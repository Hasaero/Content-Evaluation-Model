import streamlit as st
# Data Structure
import pandas as pd
import numpy as np
from ast import literal_eval
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Text Analysis
from wordcloud import WordCloud
from collections import Counter, defaultdict
from konlpy.tag import Okt
import re

# Vision
from PIL import Image

# System
import io
import os
from io import BytesIO

# Access the internet
from urllib.request import urlopen
import requests
import os

#os.chdir('C:\\Users\\7info\\Desktop\\Content_Evaluation')
# ë°ì´í„° ë¡œë”©

def plot_wordcloud(df, text_feature, font_path='BMDOHYEON_ttf.ttf'):
    plt.rc('font', family='Malgun Gothic')
    new_df = df.dropna(subset=[text_feature])
    text = ' '.join(new_df[text_feature])
    wordcloud = WordCloud(width=1200, height=800, background_color='white', font_path=font_path).generate(text)
    plt.figure(figsize=(8, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    st.pyplot(plt)
    
def calculate_token_scores(df, text_feature):
    # ê° í† í° ë³„ë¡œ score ê°’ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ìƒì„±
    new_df = df.dropna(subset=[text_feature])
    token_scores = defaultdict(list)

    # dfì˜ ê° í–‰ì„ ìˆœíšŒí•˜ë©´ì„œ
    for idx, row in new_df.iterrows():
        tokens = row[text_feature]  # í•´ë‹¹ í–‰ì˜ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
        score = row['score']  # í•´ë‹¹ í–‰ì˜ score ê°’ì„ ê°€ì ¸ì˜´
        # í† í°ì´ Noneì´ë©´ ë¬´ì‹œ
        try:
            for token in tokens:  
                token_scores[token].append(score)  # í•´ë‹¹ í† í°ì˜ score ë¦¬ìŠ¤íŠ¸ì— í˜„ì¬ score ì¶”ê°€
        except:
            continue

    # ê° í† í° ë³„ë¡œ scoreì˜ í‰ê· ì„ ê³„ì‚°
    token_avg_scores = {token: round(sum(scores) / len(scores),1) for token, scores in token_scores.items()}
    df_token_scores = pd.DataFrame(list(token_avg_scores.items()), columns=['Token', 'Average Score'])
    df_token_scores.sort_values('Average Score', ascending=False, inplace=True)
    df_token_scores.reset_index(drop=True, inplace=True)
    return df_token_scores



def remove_digit_and_single_char(text):
    cleaned_text = re.sub(r'\d+[ê°€-í£]', '', text)
    return cleaned_text

# í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬, ì œëª©ì„ í† í°í™” í•˜ëŠ” í•¨ìˆ˜
def tokenize(text):
    if pd.isna(text):
        return None
    okt = Okt()
    stopwords = ['/', '[', ']', '+', '-', '_', '=', '(', ')', '{', '}',
                 '>', '<', ':', ';', '.', ',', '?', '!', '@', '#', '$',
                 '%', '^', '&', '*', '...','"', "''"]
    
    # ì œê±°í•´ì•¼ í•˜ëŠ” ì§€ì‹œëŒ€ëª…ì‚¬
    pronouns = ["ì´","ì—¬ê¸°","ê·¸", "ì´", "ì €", "ì•„ë¬´", "ë¬´ì—‡", "ì–´ë””", "ì–¸ì œ", "ëˆ„êµ¬", "ê·¸ê±°", "ì´ë ‡ê²Œ", "ë•Œ", "ì–˜", "ë‹ˆ", "ì œ","ë„¤ê°€","ê±°", "ì´ê±°", "ë‚´", "ì´ë²ˆ", "ë„ˆ", "ë‚˜", "ì–´ëŠ", "ê²ƒ"]
    # ì œê±°í•´ì•¼ í•˜ëŠ” ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´
    meaningless = ["ì¢€","ë°" , "ë­", "ë“¯", "ë¨¸", "ë¿", "í•˜ë‹¤", "ìˆ˜", "ì–´ì©Œ", "ì˜¨", "ì•ˆ", "ë‹¤", "í¸" "ë„ˆë¬´", "ìš”", "ê²ƒ", "ë”", "ì™œ", "ëŠ”ê°€", "ê±¸", "í•¨",
                   "ì€", "ì´ë‹¤", "ìˆë‹¤", "ê²Œ", "í›„", "ë§‰", "ê·¼ë°", "ë”±", "ìª½", "ê³¼ì—°", "ì†", "ë°", "ë­˜", "ë‹¹ì‹ ", "ì˜", "ë§ˆ", "ë‚ ", "ëŸ‰", "ì–´ì©Œë‚˜", "ë˜",
                   "ì´ì  ", "ë†ˆ", "ì„¸"]
    # ì¼ë°˜ì ìœ¼ë¡œ ì œê±°í•˜ë“¯ì´ ê³ ìœ ëª…ì‚¬ë¥¼ ì œê±°í•´ì•¼ í•˜ë‚˜?
#     proper_nouns =["ìœ¨ì§€ë¬¸ë•", "ì—”í‹±ë³´ìŠ¤","ë¬¸í˜„ë¹ˆ", "ë¯¼ì´ë„¤ë‹¤ìœ¡", "05í•™ë²ˆì´ì¦ˆíˆì–´", "ì›¨ìŠ¤íŠ¸ë¸Œë£©", "ì„œì¡°", "ê°€ì˜¤ë‚˜ì‹œí‹°", "ì›°ë¦¬íë¦¬íŒŒí¬", "í„°í‹€ë¹„ì¹˜", "ì—”í‹±ë³´ìŠ¤", "ë§Œë§Œì‚¬ë‹¨", "í”¼ë¦°ì´", "ì½”-ì•¡ì‹œì–¼ ë§ˆìŠ¤í„° í¬ë¡œë…¸ë¯¸í„° í¬ë¡œë…¸ê·¸ë˜í”„", "ìˆ˜ë¶€ì§€", "ìš°ë“œìƒ·", "ì‚¼ì„±ì „ì", "ì•™ìŠ¤íƒ€", "ë¡¤í† ì²´ìŠ¤", "ì‹¤ë¹„ì§‘", "ê°€ì‹œì™€ë‹¤ìœ¡ì´ë§ˆì„", "ê³µë¯¸", "í¬í† ë¼ì´", "í—¤ë¼ë‚˜ìŠ¤", "íƒœê·¸ë¼í¬", "ì„¸ë¸ë‚˜ì´ì¸ ë ˆë³¼ë£¨ì…˜", "ë‹¤ìœ¡í’ê²½", "ì ¤ë²„ìŠ¤íŠ¸", "ì¹¼ë¦¬ìŠ¤íƒ€", "ì—ë¦¬ì¹´", "í•œë¬¸ì² ", "ì°ì‹œí™©","ë ˆì´ì»¤ìŠ¤", "ì—ìŠ¤íŒŒ", "ì½”ë‚˜", "ì…€í† ìŠ¤", "ìˆ˜ì‚¼TV", "ì¿ ë„¤ë¦¬ë·°", "ë°•êµ°", "ë¦¬ë·°ëŒ€ì¥", "í•„ì•„ì´ë¹„", "í´ì²´", "í’€ë¬¸ì–‘", "ë¦¬ë¦¬ì½”", "ì „ì„­", "ì„¸ì¸íŠ¸ë¦´ë¦¬", "ì™•ë§Œì´í˜•", "ë´‰í¬ì•…", "ë¶€ìºë¦­", "ì—í”½ì„¸ë¸", "ì‚¼ì–‘", "í‚¤ë‚˜", "ê¼°ë³´ì„", "ë¡œë²…ìŠ¤", "ì‹ ë¼ì  ", "ì í˜ˆê³µì„±ì„ ", "ìš©ëŠì‚¬ì¡°ì§", "ë®¨ë²•ì‚¬","ì—…ë¹„íŠ¸","ê¹€í†¤ìŠ¨"]    
    # ìª¼ê°°ì„ ë•Œ ì˜ë¯¸ê°€ ì†ìƒë˜ëŠ” ë‹¨ì–´
#     corruption_words = ["ê°€ì„±ë¹„","ë°©í–¥ì§€ì‹œë“±", "êµ¬ë…ì", "ì›ƒìŒë²¨", "ê¼°ëŒ€", "ë¼ì´ì§•", "í† ë¥´í…œ","ì•ŒíŠ¸ì½”ì¸", "ë¸Œì´ë¡œê·¸", "ë¯¸ë‹ˆë©€","ì¼ë°˜í†µí–‰","ì„±ì¥ì£¼", "ì˜ì…", "í—¬ë¡œìš°", "í€¸ì•„ë§", "ì‹ í™”ì¸í˜•", "ì¬ìƒì¥", "ìŠ¤í™ì—…", "í† ë¥´í…œ", "ê°ì¸í…œ", "ìˆ˜ì…ì°¨", "ë“œë¦¼ì¹´", "ì˜¬ë“œì¹´", "ì¶•ìºë¦­", "ì–‘ì¸¡", "ëŒ€í•­ë§ˆ","ì°", "ë¦¬ë¨¼ì‚¬íƒœ", "ì°¸ê°€ë¹„", "ì´ˆê°•ë ¥", "í¬ë©”ì´ì…˜", "ë¦¬í”„ë ˆì‰¬", "í†µëª¨ì§œ", "ì‹œìŒíšŒ", "ì•„ìš°í„°", "ë°ì¼ë¦¬ë£©", "ì¶œê·¼ë£©", "ìƒìê¹¡", "ì˜¬íƒ€ì„", "ê°“í…œ", "ë™í˜¸ì¸", "ì‚´ë¦¼í…œ", "ë°ì¼ë¦¬", "ììœ¨ì£¼í–‰", "ë‚„ìŠ¤ìœ—í™ˆ", "ì¶œì¡°ì ", "ë°©êµ¬ì„í† í¬","ì´ˆê°„ë‹¨", "ë¹„í•˜ì¸ë“œ", "ë°±íŒ¨í‚¹", "í´ë¡œì§•ë²¨", "í•œë°©", "ì €ì ", "ìì‘ê·¹", "ì „ì„¸ìº ", "í˜ì´ìŠ¤ë¦¬í”„íŠ¸", "ì—ì–´í”„ë¼ì´ì–´","ê¾¸ì•ˆê¾¸", "ë¬´ê³¼ê¸ˆ","ë¦¬ë·°", "ê¿€ì¡°í•©","ì½¤ë°±í™ˆ", "íˆ¬í•¸ë“œ", "í‚¹í”¼ìŠ¤", "í…œíŠ¸ë¦¬","í˜¼ì¡°ì„¸", "ë² ì–´ë§ˆì¼“ë ë¦¬"]

#     # ë³¸ê²©ì ìœ¼ë¡œ ì „ì²˜ë¦¬ë¥¼ í•˜ê¸°ì „ ìª¼ê°œì§€ ë§ì•„ì•¼ í•˜ëŠ” ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë¥¼ ì €ì¥í•˜ëŠ” ë¦¬ìŠ¤íŠ¸
#     keep_words = []
#     # ë‹¨ì–´ë¥¼ ìª¼ê°œê¸° ì „ corruption_wordsì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë“¤ì´ ë¬¸ì¥ ë‚´ ìˆìœ¼ë©´ ì¶”ì¶œ
#     for corruption_word in corruption_words:
#         if corruption_word in text:
#             keep_words.append(corruption_word)
#             text = text.replace(corruption_word, " ")
    # 2307íšŒë“± (ìˆ«ì+ëª…ì‚¬)ëœ ê²½ìš° ì „ì²´ ì œê±°         
    text=remove_digit_and_single_char(text)
    # ì •ê·œí™” ë° ì–´ê·¼ ì¶”ì¶œ
    words = okt.pos(text, norm=True, stem=True)  
    # í’ˆì‚¬ íƒœê·¸ ì„ íƒ
    words = [word for word, pos in words if pos in ['Noun', 'Adjective', 'Verb', 'Adverb']]  # ì„ íƒí•œ í’ˆì‚¬ë§Œ ì¶”ì¶œ
    # ë¶ˆìš©ì–´, ì§€ì‹œëŒ€ëª…ì‚¬, ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´, ê³ ìœ ëª…ì‚¬ ì œê±°
    words = [word for word in words if word not in stopwords + pronouns + meaningless] 
    # ìµœì¢… ë‹¨ì–´ì™€ ì‚¬ì „ì— ë¶„ë¦¬í•´ë‘” ìª¼ê°œì§€ ë§ì•„ì•¼ í•˜ëŠ” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ í•©í•œë‹¤.
    # words = words + keep_words
    
    return words

def plot_freq_keyword(df, text_feature):
    plt.rc('font', family='Malgun Gothic')
    new_df = df.dropna(subset=[text_feature])
    df_token_scores = calculate_token_scores(new_df, text_feature)
    df_token_scores.set_index('Token', inplace=True)
    
    words = [item for sublist in new_df[text_feature].tolist() for item in sublist]
    # ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ê³„ì‚°
    word_freq = Counter(words)
    # ìƒìœ„ 10ê°œ ë‹¨ì–´ë§Œ ì„ íƒ
    top_10 = word_freq.most_common(10)
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    word_df = pd.DataFrame(top_10, columns=['Word', 'Frequency'])
    # ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(12, 5))
    word_df.plot(kind='bar', x='Word', y='Frequency', color='orange')
    plt.xticks(rotation=0)
    plt.xlabel("")
    plt.yticks([])  # Hide the x-axis tick labels
    plt.legend().set_visible(False)
    st.pyplot(plt)
    result_df = df_token_scores.loc[word_df['Word']].T
    result_df.index = ['í‰ê·  ì ìˆ˜']
    return result_df
    # top_10ì— í•´ë‹¹í•˜ëŠ” í† í°ë“¤ì˜ í‰ê·  ì ìˆ˜ ì¶œë ¥

def convert_time(minutes):
    minutes_int = int(minutes)
    seconds = int((minutes - minutes_int) * 60)
    time_str = f"{minutes_int}ë¶„ {seconds}ì´ˆ"
    return time_str

def make_one_str(df, feature):
    tmp_df = df.dropna(subset=feature)
    all_tags = []
    list_col = tmp_df[feature].apply(literal_eval)
    for tag_list in list_col:
        all_tags.extend(tag_list)
        
    all_tags_str = ' '.join(all_tags)
    all_tags_str = all_tags_str.replace('&', ' ')
    return all_tags, all_tags_str

def color_print(color_df, genre_eng, feature):
    color_value = color_df[color_df['genre'] == genre_eng][feature]
    color_value = color_value.iloc[0]
    if color_value == "Very High":
        return "ë§¤ìš° ë†’ì•„ìš”"
    elif color_value == "High":
        return "ë†’ì•„ìš”"
    elif color_value == "Medium":
        return "ë‚®ì•„ìš”"
    elif color_value == "Low":
        return "ë§¤ìš° ë‚®ì•„ìš”"  
    
# df = pd.read_csv('good_ad_data.csv')
# color_df = pd.read_csv('good_ad_color.csv')
df = pd.read_csv('https://raw.githubusercontent.com/Hasaero/Content-Evaluation-Model/master/good_ad_data.csv')
color_df = pd.read_csv('https://raw.githubusercontent.com/Hasaero/Content-Evaluation-Model/master/good_ad_color.csv')
df['title_token'] = df['title'].apply(tokenize)
df['thumbnail_text_token'] = df['thumbnail_text'].apply(tokenize)
# í•œê¸€í™”ë¥¼ ìœ„í•œ ì¥ë¥´ ë”•ì…”ë„ˆë¦¬
genre_dict = {
#'ë™ë¬¼': 'Pets & Animals',
 'ìë™ì°¨': 'Autos & Vehicles',
 'ì¼ìƒ': 'People & Blogs',
 'ë°©ë²• & ìŠ¤íƒ€ì¼': 'Howto & Style',
 'ì—¬í–‰': 'Travel & Events',
 'ìŒì•…': 'Music',
 'ê²Œì„': 'Gaming',
 'êµìœ¡': 'Education',
 #'ê³¼í•™ & ê¸°ìˆ ': 'Science & Technology',
 'ì—”í„°í…Œì¸ë¨¼íŠ¸': 'Entertainment',
 'ì½”ë¯¸ë””': 'Comedy',
 'ìŠ¤í¬ì¸ ': 'Sports',
 #'ë‰´ìŠ¤ & ì •ì¹˜': 'News & Politics',
 #'ì˜í™” & ì• ë‹ˆë©”ì´ì…˜': 'Film & Animation'
 }

emoticon_dict = {'ë™ë¬¼': 'ğŸ¶',
 'ìë™ì°¨': 'ğŸš“',
 'ì¼ìƒ': 'ğŸ˜€',
 'ë°©ë²• & ìŠ¤íƒ€ì¼': 'ğŸª',
 'ì—¬í–‰': 'ğŸ›«',
 'ìŒì•…': 'ğŸµ',
 'ê²Œì„': 'ğŸ®',
 'êµìœ¡': 'ğŸ«',
 'ê³¼í•™ & ê¸°ìˆ ': 'ğŸ‘¨â€ğŸ’»',
 'ì—”í„°í…Œì¸ë¨¼íŠ¸': 'ğŸ“º',
 'ì½”ë¯¸ë””': 'ğŸ˜‚',
 'ìŠ¤í¬ì¸ ': 'âš½',
 'ë‰´ìŠ¤ & ì •ì¹˜': 'ğŸ“°',
 'ì˜í™” & ì• ë‹ˆë©”ì´ì…˜': 'ğŸ¿'}

# ì¥ë¥´ ì„ íƒ (í•œê¸€ë¡œ í‘œì‹œ)
if 'genre' not in st.session_state:
    st.session_state['genre'] = None

page = st.sidebar.selectbox("ì–´ë–¤ íŠ¹ì§•ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?", ['í™ˆ', 'ì–´ë–¤ ì œëª©ì´ ì¸ê¸°ê°€ ë§ì„ê¹Œ?', 'ì´ëª©ì„ ë„ëŠ” ì¸ë„¤ì¼!', 'ê´‘ê³  ì˜ìƒì„ ì˜ ë§Œë“œë ¤ë©´?'])
if page == 'í™ˆ':
    st.markdown(
    "*Handong Global University - Big Data Analysis 2023-01*"
    )
    # # ë¡œê³  ì´ë¯¸ì§€ ë¡œë“œ
    # image = Image.open('logo.jpg')
    # st.image("logo.jpg", width=180)

    # ìƒë‹¨ ì œëª©
    st.subheader('ğŸ¥ ìì‹ ì—ê²Œ ë§ëŠ” ê´‘ê³ ì˜ìƒ íŠ¹ì§•ì„ ëª¨ì•„ë³´ì„¸ìš”!')

    # ì¥ë¥´ ì„ íƒ (í•œê¸€ë¡œ í‘œì‹œ)
    genre_kor = st.selectbox('ì¥ë¥´ë¥¼ ì„ íƒí•˜ì„¸ìš”.', [None]+list(genre_dict.keys()))
    st.sidebar.markdown(f"í˜„ì¬ ì„ íƒëœ ì¥ë¥´ëŠ” **{genre_kor}** ì´ì—ìš”.")
    
    # ì¥ë¥´ ì„ íƒ ë¦¬ìŠ¤íŠ¸
    if genre_kor is not None:
        genre_eng = genre_dict[genre_kor]

        # ì¥ë¥´ì— ë”°ë¥¸ ë°ì´í„° í•„í„°ë§
        grouped_df = df.groupby('genre')
        genre_df = grouped_df.get_group(genre_eng)
        st.session_state['genre'] = (genre_kor, grouped_df.get_group(genre_eng))
        all_tags, all_tags_str = make_one_str(genre_df, 'tag')
        # ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ê³„ì‚°
        word_freq = Counter(all_tags)
        # ìƒìœ„ 10ê°œ ë‹¨ì–´ë§Œ ì„ íƒ
        top_5 = word_freq.most_common(10)
        keywords = [item[0] for item in top_5]
        keywords_str = ", ".join(keywords)
        st.markdown(
        f'<p style="color:orange;"><strong>ğŸ’¡ í•˜ì´í”ˆì€ {genre_kor}ì—ì„œ "{keywords_str}"ì˜ íƒœê·¸ë¥¼ ë°œê²¬í–ˆì–´ìš”!</strong></p>',
        unsafe_allow_html=True,
        )
        st.success(emoticon_dict[genre_kor] +' '+ f"**ì™¼ìª½ ë©”ë‰´ì—ì„œ '{genre_kor}' ê´‘ê³  ì˜ìƒì˜ íŠ¹ì§•ì„ ê³¨ë¼ë³´ì„¸ìš”.**")

elif page == 'ì–´ë–¤ ì œëª©ì´ ì¸ê¸°ê°€ ë§ì„ê¹Œ?':
    if st.session_state['genre'] is not None:
        with st.spinner('**ì˜ìƒë“¤ì„ ë¶„ì„í•˜ê³  ìˆì–´ìš”...**'):
            genre_kor, genre_df = st.session_state['genre']
            st.sidebar.markdown(f"í˜„ì¬ ì„ íƒëœ ì¥ë¥´ëŠ” **{genre_kor}** ì´ì—ìš”.")
            st.title(emoticon_dict[genre_kor] + ' '+ f"'{genre_kor}' ì¥ë¥´ëŠ”...")
            st.markdown("<hr>", unsafe_allow_html=True)
            st.subheader("ğŸ”¤ ì˜ìƒ ì œëª©ì— ì´ëŸ¬í•œ í‚¤ì›Œë“œê°€ ë§ì•„ìš”.")
            plot_wordcloud(genre_df, text_feature='title')
            
            st.markdown("<hr>", unsafe_allow_html=True)
            st.subheader(f"âœï¸ ì˜ìƒ ì œëª©ì— ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œì—ìš”.")
            freq_words_df = plot_freq_keyword(genre_df, 'title_token')
            st.subheader("ğŸ‘‡ í‚¤ì›Œë“œì— ëŒ€í•œ ì ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!")
            st.info(f"**ì ìˆ˜ = ì˜ìƒì¡°íšŒìˆ˜/ì±„ë„í‰ê· ì¡°íšŒìˆ˜ ì˜ í‰ê· **")
            st.write(freq_words_df)
        
elif page == 'ì´ëª©ì„ ë„ëŠ” ì¸ë„¤ì¼!':
    if st.session_state['genre'] is not None:
        with st.spinner('**ì˜ìƒë“¤ì„ ë¶„ì„í•˜ê³  ìˆì–´ìš”...**'):
            genre_kor, genre_df = st.session_state['genre']
            genre_eng = genre_dict[genre_kor]
            st.sidebar.markdown(f"í˜„ì¬ ì„ íƒëœ ì¥ë¥´ëŠ” **{genre_kor}** ì´ì—ìš”.")
            st.title(emoticon_dict[genre_kor] + ' '+ f"'{genre_kor}' ì¥ë¥´ëŠ”...")
            st.markdown("<hr>", unsafe_allow_html=True)
            st.subheader(f"ğŸ“ ì¸ë„¤ì¼ì—ì„œ ë¬¸ì ì˜ì—­ì´ {round(genre_df['thumbnail_text_ratio'].mean()*100)}% ë¥¼ ì°¨ì§€í•´ìš”.")
            st.info(f"**ì¸ë„¤ì¼ì˜ ë¬¸ìì˜ì—­ì€ í‰ê· ì ìœ¼ë¡œ {round(df['thumbnail_text_ratio'].mean()*100)}% ì—ìš”.**")
            st.markdown("<hr>", unsafe_allow_html=True)
            ### ìƒ‰ê¹” ì •ë³´
            st.subheader(f"ğŸŒˆ ì¸ë„¤ì¼ì—ì„œ ìƒ‰ìƒ, ëª…ë„, ì±„ë„ë¥¼ ì‚´í´ë´ìš”.")
            st.subheader(f"ğŸŸ ì¸ë„¤ì¼ì˜ ìƒ‰ìƒì´ {color_print(color_df, genre_eng, 'color_category')}")
            st.subheader(f"ğŸŸ¡ ì¸ë„¤ì¼ì˜ ëª…ë„ê°€ {color_print(color_df, genre_eng, 'lightness_category')}")
            st.subheader(f"ğŸŸ¢ ì¸ë„¤ì¼ì˜ ì±„ë„ê°€ {color_print(color_df, genre_eng, 'saturation_category')}")
            st.info("**ì „ì²´ ì±„ë„ì˜ ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„**")
            st.markdown("<hr>", unsafe_allow_html=True)
            
            st.subheader(f"âœï¸ ì¸ë„¤ì¼ì— ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œì—ìš”.")
            freq_words_df = plot_freq_keyword(genre_df, 'thumbnail_text_token') 
            st.subheader("ğŸ‘‡ í‚¤ì›Œë“œì— ëŒ€í•œ ì ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!")
            st.info(f"**ì ìˆ˜ = ì˜ìƒì¡°íšŒìˆ˜/ì±„ë„í‰ê· ì¡°íšŒìˆ˜ ì˜ í‰ê· **")
            st.write(freq_words_df)
            all_tags, all_tags_str = make_one_str(genre_df, 'thumbnail_labels_translate')
            
            st.markdown("<hr>", unsafe_allow_html=True)
            st.subheader("ğŸ“· ì¸ë„¤ì¼ì—ì„œ íƒì§€ëœ ê°ì²´ë“¤ì„ ë³´ì—¬ë“œë¦´ê²Œìš”.")
            wordcloud = WordCloud(width=1200, height=800, background_color='white', font_path='BMDOHYEON_ttf.ttf').generate(all_tags_str)
            plt.figure(figsize=(8, 8))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            st.pyplot(plt)
elif page == 'ê´‘ê³  ì˜ìƒì„ ì˜ ë§Œë“œë ¤ë©´?':
    if st.session_state['genre'] is not None:
        genre_kor, genre_df = st.session_state['genre']
        st.sidebar.markdown(f"í˜„ì¬ ì„ íƒëœ ì¥ë¥´ëŠ” **{genre_kor}** ì´ì—ìš”.")
        duration_dist = genre_df['duration_min'].describe()
        mean_time = duration_dist['mean']
        genre_ratio = round((len(genre_df)/len(df)) * 100)
        st.title(emoticon_dict[genre_kor] + ' '+ f'{genre_kor} ì¥ë¥´ëŠ”...')
        st.markdown("<hr>", unsafe_allow_html=True)
        st.subheader(f"ğŸ” ì¸ê¸°ìˆëŠ” ìœ ë£Œê´‘ê³  ì˜ìƒ ì¤‘ {genre_ratio}% ë¥¼ ì°¨ì§€í•˜ê³  ìˆì–´ìš”.")
        st.markdown("<hr>", unsafe_allow_html=True)
        st.subheader("â±ï¸ í‰ê·  ì˜ìƒ ê¸¸ì´ëŠ” " + convert_time(mean_time)+ "ì—ìš”.")
        st.info(f"**ì „ì²´ ì˜ìƒ í‰ê·  ê¸¸ì´ëŠ” {convert_time(df['duration_min'].mean())}ì—ìš”.**")
        
        st.subheader(f"ğŸ’¯ ì˜ìƒë“¤ì˜ í‰ê·  ì ìˆ˜ëŠ” {round(genre_df['score'].mean(),2)} ì ì´ì—ìš”.")
        st.info(f"**ì „ì²´ ì˜ìƒ í‰ê·  ì ìˆ˜ëŠ” {round(df['score'].mean(),2)} ì ì´ì—ìš”.**")
        st.info(f"**ì ìˆ˜ = ì˜ìƒì¡°íšŒìˆ˜/ì±„ë„í‰ê· ì¡°íšŒìˆ˜ ì˜ í‰ê· **")


