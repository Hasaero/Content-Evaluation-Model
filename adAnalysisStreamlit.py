import streamlit as st
# Data Structure
import pandas as pd
import numpy as np
from ast import literal_eval
# Visualization
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns

# Text Analysis
from wordcloud import WordCloud
from collections import Counter, defaultdict
import re

# Vision
from PIL import Image

# System
import io
import os
from io import BytesIO

# Access the internet
from urllib.request import urlopen
import requests
import os


# font_dir = 'https://raw.githubusercontent.com/Hasaero/Content-Evaluation-Model/master/BMDOHYEON_ttf.ttf'
# fm.fontManager.addfont(font_dir)
# fm._load_fontmanager(try_read_cache=False)
# plt.rc('font', family='BM Dohyeon')
url = 'https://raw.githubusercontent.com/Hasaero/Content-Evaluation-Model/master/BMDOHYEON_ttf.ttf'
response = requests.get(url)

# í˜„ì¬ ë””ë ‰í† ë¦¬ì— ttf íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.
with open('BMDOHYEON_ttf.ttf', 'wb') as out_file:
    out_file.write(response.content)

# ì´ì œ íŒŒì¼ì€ ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ft2font.FT2Fontì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
font_path = os.path.abspath('BMDOHYEON_ttf.ttf')
fm.fontManager.addfont(font_path)

# ìœ„ ì½”ë“œëŠ” ìºì‹œëœ FontManagerë¥¼ ë¬´ì‹œí•˜ê³  ìƒˆë¡œìš´ ê²ƒì„ ë¶ˆëŸ¬ì˜¤ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
fm._load_fontmanager(try_read_cache=False)

# ì´ì œ 'BM Dohyeon' í°íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ëìŠµë‹ˆë‹¤.
plt.rc('font', family='BM Dohyeon')
#os.chdir('C:\\Users\\7info\\Desktop\\Content_Evaluation')
# ë°ì´í„° ë¡œë”©

def plot_wordcloud(df, text_feature, font_path=url):
    
    new_df = df.dropna(subset=[text_feature])
    text = ' '.join(new_df[text_feature])
    wordcloud = WordCloud(width=1200, height=800, background_color='white', font_path='BMDOHYEON_ttf.ttf').generate(text)
    plt.figure(figsize=(8, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    st.pyplot(plt)
    
def calculate_token_scores(df, text_feature):
    # ê° í† í° ë³„ë¡œ score ê°’ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ìƒì„±
    new_df = df.dropna(subset=[text_feature])
    token_scores = defaultdict(list)

    # dfì˜ ê° í–‰ì„ ìˆœíšŒí•˜ë©´ì„œ
    for idx, row in new_df.iterrows():
        tokens = row[text_feature]  # í•´ë‹¹ í–‰ì˜ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
        score = row['score']  # í•´ë‹¹ í–‰ì˜ score ê°’ì„ ê°€ì ¸ì˜´
        # í† í°ì´ Noneì´ë©´ ë¬´ì‹œ
        try:
            for token in tokens:  
                token_scores[token].append(score)  # í•´ë‹¹ í† í°ì˜ score ë¦¬ìŠ¤íŠ¸ì— í˜„ì¬ score ì¶”ê°€
        except:
            continue

    # ê° í† í° ë³„ë¡œ scoreì˜ í‰ê· ì„ ê³„ì‚°
    token_avg_scores = {token: round(sum(scores) / len(scores),1) for token, scores in token_scores.items()}
    df_token_scores = pd.DataFrame(list(token_avg_scores.items()), columns=['Token', 'Average Score'])
    df_token_scores.sort_values('Average Score', ascending=False, inplace=True)
    df_token_scores.reset_index(drop=True, inplace=True)
    return df_token_scores



def plot_freq_keyword(df, text_feature):
    
    new_df = df.dropna(subset=[text_feature])
    df_token_scores = calculate_token_scores(new_df, text_feature)
    df_token_scores.set_index('Token', inplace=True)
    
    words = [item for sublist in new_df[text_feature].tolist() for item in sublist]
    # ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ê³„ì‚°
    word_freq = Counter(words)
    # ìƒìœ„ 10ê°œ ë‹¨ì–´ë§Œ ì„ íƒ
    top_10 = word_freq.most_common(10)
    # ë°ì´í„°í”„ë ˆì„ ìƒì„±
    word_df = pd.DataFrame(top_10, columns=['Word', 'Frequency'])
    # ë§‰ëŒ€ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(12, 5))
    word_df.plot(kind='bar', x='Word', y='Frequency', color='orange')
    plt.xticks(rotation=0)
    plt.xlabel("")
    plt.yticks([])  # Hide the x-axis tick labels
    plt.legend().set_visible(False)
    st.pyplot(plt)
    result_df = df_token_scores.loc[word_df['Word']].T
    result_df.index = ['í‰ê·  ì ìˆ˜']
    return result_df
    # top_10ì— í•´ë‹¹í•˜ëŠ” í† í°ë“¤ì˜ í‰ê·  ì ìˆ˜ ì¶œë ¥

def convert_time(minutes):
    minutes_int = int(minutes)
    seconds = int((minutes - minutes_int) * 60)
    time_str = f"{minutes_int}ë¶„ {seconds}ì´ˆ"
    return time_str

def make_one_str(df, feature):
    tmp_df = df.dropna(subset=feature)
    all_tags = []
    list_col = tmp_df[feature].apply(literal_eval)
    for tag_list in list_col:
        all_tags.extend(tag_list)
        
    all_tags_str = ' '.join(all_tags)
    all_tags_str = all_tags_str.replace('&', ' ')
    return all_tags, all_tags_str

def color_print(color_df, genre_eng, feature):
    color_value = color_df[color_df['genre'] == genre_eng][feature]
    color_value = color_value.iloc[0]
    if color_value == "Very High":
        return "ë§¤ìš° ë†’ì•„ìš”"
    elif color_value == "High":
        return "ë†’ì•„ìš”"
    elif color_value == "Medium":
        return "ë‚®ì•„ìš”"
    elif color_value == "Low":
        return "ë§¤ìš° ë‚®ì•„ìš”"  

def convert_to_list(val):
    try:
        return literal_eval(val)
    except (ValueError, SyntaxError):
        return None  # or whatever you want to return for invalid values
    
# df = pd.read_csv('good_ad_data.csv')
# color_df = pd.read_csv('good_ad_color.csv')
df = pd.read_csv('https://raw.githubusercontent.com/Hasaero/Content-Evaluation-Model/master/good_ad_data.csv')
df['title_token'] = df['title_token'].apply(convert_to_list)
df['thumbnail_text_token'] = df['thumbnail_text_token'].apply(convert_to_list)

color_df = pd.read_csv('https://raw.githubusercontent.com/Hasaero/Content-Evaluation-Model/master/good_ad_color.csv')

# í•œê¸€í™”ë¥¼ ìœ„í•œ ì¥ë¥´ ë”•ì…”ë„ˆë¦¬
genre_dict = {
#'ë™ë¬¼': 'Pets & Animals',
 'ìë™ì°¨': 'Autos & Vehicles',
 'ì¼ìƒ': 'People & Blogs',
 'ë°©ë²• & ìŠ¤íƒ€ì¼': 'Howto & Style',
 'ì—¬í–‰': 'Travel & Events',
 'ìŒì•…': 'Music',
 'ê²Œì„': 'Gaming',
 'êµìœ¡': 'Education',
 #'ê³¼í•™ & ê¸°ìˆ ': 'Science & Technology',
 'ì—”í„°í…Œì¸ë¨¼íŠ¸': 'Entertainment',
 'ì½”ë¯¸ë””': 'Comedy',
 'ìŠ¤í¬ì¸ ': 'Sports',
 #'ë‰´ìŠ¤ & ì •ì¹˜': 'News & Politics',
 #'ì˜í™” & ì• ë‹ˆë©”ì´ì…˜': 'Film & Animation'
 }

emoticon_dict = {'ë™ë¬¼': 'ğŸ¶',
 'ìë™ì°¨': 'ğŸš“',
 'ì¼ìƒ': 'ğŸ˜€',
 'ë°©ë²• & ìŠ¤íƒ€ì¼': 'ğŸª',
 'ì—¬í–‰': 'ğŸ›«',
 'ìŒì•…': 'ğŸµ',
 'ê²Œì„': 'ğŸ®',
 'êµìœ¡': 'ğŸ«',
 'ê³¼í•™ & ê¸°ìˆ ': 'ğŸ‘¨â€ğŸ’»',
 'ì—”í„°í…Œì¸ë¨¼íŠ¸': 'ğŸ“º',
 'ì½”ë¯¸ë””': 'ğŸ˜‚',
 'ìŠ¤í¬ì¸ ': 'âš½',
 'ë‰´ìŠ¤ & ì •ì¹˜': 'ğŸ“°',
 'ì˜í™” & ì• ë‹ˆë©”ì´ì…˜': 'ğŸ¿'}

# ì¥ë¥´ ì„ íƒ (í•œê¸€ë¡œ í‘œì‹œ)
if 'genre' not in st.session_state:
    st.session_state['genre'] = None

page = st.sidebar.selectbox("ì–´ë–¤ íŠ¹ì§•ì„ ì°¾ìœ¼ì‹œë‚˜ìš”?", ['í™ˆ', 'ì–´ë–¤ ì œëª©ì´ ì¸ê¸°ê°€ ë§ì„ê¹Œ?', 'ì´ëª©ì„ ë„ëŠ” ì¸ë„¤ì¼!', 'ê´‘ê³  ì˜ìƒì„ ì˜ ë§Œë“œë ¤ë©´?'])
if page == 'í™ˆ':
    st.markdown(
    "*Handong Global University - Big Data Analysis 2023-01*"
    )
    # # ë¡œê³  ì´ë¯¸ì§€ ë¡œë“œ
    # image = Image.open('logo.jpg')
    # st.image("logo.jpg", width=180)

    # ìƒë‹¨ ì œëª©
    st.subheader('ğŸ¥ ìì‹ ì—ê²Œ ë§ëŠ” ê´‘ê³ ì˜ìƒ íŠ¹ì§•ì„ ëª¨ì•„ë³´ì„¸ìš”!')

    # ì¥ë¥´ ì„ íƒ (í•œê¸€ë¡œ í‘œì‹œ)
    genre_kor = st.selectbox('ì¥ë¥´ë¥¼ ì„ íƒí•˜ì„¸ìš”.', [None]+list(genre_dict.keys()))
    st.sidebar.markdown(f"í˜„ì¬ ì„ íƒëœ ì¥ë¥´ëŠ” **{genre_kor}** ì´ì—ìš”.")
    
    # ì¥ë¥´ ì„ íƒ ë¦¬ìŠ¤íŠ¸
    if genre_kor is not None:
        genre_eng = genre_dict[genre_kor]

        # ì¥ë¥´ì— ë”°ë¥¸ ë°ì´í„° í•„í„°ë§
        grouped_df = df.groupby('genre')
        genre_df = grouped_df.get_group(genre_eng)
        st.session_state['genre'] = (genre_kor, grouped_df.get_group(genre_eng))
        all_tags, all_tags_str = make_one_str(genre_df, 'tag')
        # ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ ê³„ì‚°
        word_freq = Counter(all_tags)
        # ìƒìœ„ 10ê°œ ë‹¨ì–´ë§Œ ì„ íƒ
        top_5 = word_freq.most_common(10)
        keywords = [item[0] for item in top_5]
        keywords_str = ", ".join(keywords)
        st.markdown(
        f'<p style="color:orange;"><strong>ğŸ’¡ í•˜ì´í”ˆì€ {genre_kor}ì—ì„œ "{keywords_str}"ì˜ íƒœê·¸ë¥¼ ë°œê²¬í–ˆì–´ìš”!</strong></p>',
        unsafe_allow_html=True,
        )
        st.success(emoticon_dict[genre_kor] +' '+ f"**ì™¼ìª½ ë©”ë‰´ì—ì„œ '{genre_kor}' ê´‘ê³  ì˜ìƒì˜ íŠ¹ì§•ì„ ê³¨ë¼ë³´ì„¸ìš”.**")

elif page == 'ì–´ë–¤ ì œëª©ì´ ì¸ê¸°ê°€ ë§ì„ê¹Œ?':
    if st.session_state['genre'] is not None:
        with st.spinner('**ì˜ìƒë“¤ì„ ë¶„ì„í•˜ê³  ìˆì–´ìš”...**'):
            genre_kor, genre_df = st.session_state['genre']
            st.sidebar.markdown(f"í˜„ì¬ ì„ íƒëœ ì¥ë¥´ëŠ” **{genre_kor}** ì´ì—ìš”.")
            st.title(emoticon_dict[genre_kor] + ' '+ f"'{genre_kor}' ì¥ë¥´ëŠ”...")
            st.markdown("<hr>", unsafe_allow_html=True)
            st.subheader("ğŸ”¤ ì˜ìƒ ì œëª©ì— ì´ëŸ¬í•œ í‚¤ì›Œë“œê°€ ë§ì•„ìš”.")
            plot_wordcloud(genre_df, text_feature='title')
            
            st.markdown("<hr>", unsafe_allow_html=True)
            st.subheader(f"âœï¸ ì˜ìƒ ì œëª©ì— ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œì—ìš”.")
            freq_words_df = plot_freq_keyword(genre_df, 'title_token')
            st.subheader("ğŸ‘‡ í‚¤ì›Œë“œì— ëŒ€í•œ ì ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!")
            st.info(f"**ì ìˆ˜ = ì˜ìƒì¡°íšŒìˆ˜/ì±„ë„í‰ê· ì¡°íšŒìˆ˜ ì˜ í‰ê· **")
            st.write(freq_words_df)
        
elif page == 'ì´ëª©ì„ ë„ëŠ” ì¸ë„¤ì¼!':
    if st.session_state['genre'] is not None:
        with st.spinner('**ì˜ìƒë“¤ì„ ë¶„ì„í•˜ê³  ìˆì–´ìš”...**'):
            genre_kor, genre_df = st.session_state['genre']
            genre_eng = genre_dict[genre_kor]
            st.sidebar.markdown(f"í˜„ì¬ ì„ íƒëœ ì¥ë¥´ëŠ” **{genre_kor}** ì´ì—ìš”.")
            st.title(emoticon_dict[genre_kor] + ' '+ f"'{genre_kor}' ì¥ë¥´ëŠ”...")
            st.markdown("<hr>", unsafe_allow_html=True)
            st.subheader(f"ğŸ“ ì¸ë„¤ì¼ì—ì„œ ë¬¸ì ì˜ì—­ì´ {round(genre_df['thumbnail_text_ratio'].mean()*100)}% ë¥¼ ì°¨ì§€í•´ìš”.")
            st.info(f"**ì¸ë„¤ì¼ì˜ ë¬¸ìì˜ì—­ì€ í‰ê· ì ìœ¼ë¡œ {round(df['thumbnail_text_ratio'].mean()*100)}% ì—ìš”.**")
            st.markdown("<hr>", unsafe_allow_html=True)
            ### ìƒ‰ê¹” ì •ë³´
            st.subheader(f"ğŸŒˆ ì¸ë„¤ì¼ì—ì„œ ìƒ‰ìƒ, ëª…ë„, ì±„ë„ë¥¼ ì‚´í´ë´ìš”.")
            st.subheader(f"ğŸŸ ì¸ë„¤ì¼ì˜ ìƒ‰ìƒì´ {color_print(color_df, genre_eng, 'color_category')}")
            st.subheader(f"ğŸŸ¡ ì¸ë„¤ì¼ì˜ ëª…ë„ê°€ {color_print(color_df, genre_eng, 'lightness_category')}")
            st.subheader(f"ğŸŸ¢ ì¸ë„¤ì¼ì˜ ì±„ë„ê°€ {color_print(color_df, genre_eng, 'saturation_category')}")
            st.info("**ì „ì²´ ì±„ë„ì˜ ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„**")
            st.markdown("<hr>", unsafe_allow_html=True)
            
            st.subheader(f"âœï¸ ì¸ë„¤ì¼ì— ìì£¼ ë“±ì¥í•˜ëŠ” í‚¤ì›Œë“œì—ìš”.")
            freq_words_df = plot_freq_keyword(genre_df, 'thumbnail_text_token') 
            st.subheader("ğŸ‘‡ í‚¤ì›Œë“œì— ëŒ€í•œ ì ìˆ˜ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”!")
            st.info(f"**ì ìˆ˜ = ì˜ìƒì¡°íšŒìˆ˜/ì±„ë„í‰ê· ì¡°íšŒìˆ˜ ì˜ í‰ê· **")
            st.write(freq_words_df)
            all_tags, all_tags_str = make_one_str(genre_df, 'thumbnail_labels_translate')
            
            st.markdown("<hr>", unsafe_allow_html=True)
            st.subheader("ğŸ“· ì¸ë„¤ì¼ì—ì„œ íƒì§€ëœ ê°ì²´ë“¤ì„ ë³´ì—¬ë“œë¦´ê²Œìš”.")
            wordcloud = WordCloud(width=1200, height=800, background_color='white', font_path='BMDOHYEON_ttf.ttf').generate(all_tags_str)
            plt.figure(figsize=(8, 8))
            plt.imshow(wordcloud, interpolation='bilinear')
            plt.axis('off')
            st.pyplot(plt)
elif page == 'ê´‘ê³  ì˜ìƒì„ ì˜ ë§Œë“œë ¤ë©´?':
    if st.session_state['genre'] is not None:
        genre_kor, genre_df = st.session_state['genre']
        st.sidebar.markdown(f"í˜„ì¬ ì„ íƒëœ ì¥ë¥´ëŠ” **{genre_kor}** ì´ì—ìš”.")
        duration_dist = genre_df['duration_min'].describe()
        mean_time = duration_dist['mean']
        genre_ratio = round((len(genre_df)/len(df)) * 100)
        st.title(emoticon_dict[genre_kor] + ' '+ f'{genre_kor} ì¥ë¥´ëŠ”...')
        st.markdown("<hr>", unsafe_allow_html=True)
        st.subheader(f"ğŸ” ì¸ê¸°ìˆëŠ” ìœ ë£Œê´‘ê³  ì˜ìƒ ì¤‘ {genre_ratio}% ë¥¼ ì°¨ì§€í•˜ê³  ìˆì–´ìš”.")
        st.markdown("<hr>", unsafe_allow_html=True)
        st.subheader("â±ï¸ í‰ê·  ì˜ìƒ ê¸¸ì´ëŠ” " + convert_time(mean_time)+ "ì—ìš”.")
        st.info(f"**ì „ì²´ ì˜ìƒ í‰ê·  ê¸¸ì´ëŠ” {convert_time(df['duration_min'].mean())}ì—ìš”.**")
        
        st.subheader(f"ğŸ’¯ ì˜ìƒë“¤ì˜ í‰ê·  ì ìˆ˜ëŠ” {round(genre_df['score'].mean(),2)} ì ì´ì—ìš”.")
        st.info(f"**ì „ì²´ ì˜ìƒ í‰ê·  ì ìˆ˜ëŠ” {round(df['score'].mean(),2)} ì ì´ì—ìš”.**")
        st.info(f"**ì ìˆ˜ = ì˜ìƒì¡°íšŒìˆ˜/ì±„ë„í‰ê· ì¡°íšŒìˆ˜ ì˜ í‰ê· **")


